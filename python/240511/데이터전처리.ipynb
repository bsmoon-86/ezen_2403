{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 설치 \n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 범주형 데이터 처리 \n",
    "- 일반적인 데이터 모델링은 수학적 연산형 모델링이 대부분\n",
    "- 범주형 데이터가 사용이 불가능\n",
    "- 범수형 데이터를 더미 변수를 이용하여 수학적 데이터로 변환\n",
    "- 더미변수 -> 범주형 데이터들을 각각 컬럼의 이름으로 생성 -> 해당 범주에 속하는가? False(0), True(1)로 표현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.DataFrame(\n",
    "    load_wine()['data'], \n",
    "    columns = load_wine()['feature_names']\n",
    ")\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine['class'] = load_wine()['target']\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주형 데이터로 변환\n",
    "wine['class'] = wine['class'].map(\n",
    "    {\n",
    "        0 : 'class_0', \n",
    "        1 : 'class_1', \n",
    "        2 : 'class_2'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class 컬럼의 데이터들을 더미변수 생성\n",
    "# pandas 안에 있는 get_dummies() 함수를 이용\n",
    "wine_dummy = pd.get_dummies(\n",
    "    wine, \n",
    "    columns = ['class']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_dummy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터의 분할 \n",
    "- 분석 모델을 학습시키고 성과를 확인하기 위해 기존의 데이터 셋을 나눠주는 작업 \n",
    "- 데이터를 학습 데이터(train), 시험 데이터(test)로 나눠주고 데이터들을 일정 비율로 나눠주는 작업 \n",
    "- 일반적인 비율\n",
    "    - Train : Test -> 7 : 3\n",
    "- sklearn에 내장된 train_test_split() 함수를 이용\n",
    "    - train_test_split(X, Y, test_size = None, random_state = None, shuffle = bool, stratify = None)\n",
    "        - X : 독립 변수 \n",
    "        - Y : 종속 변수\n",
    "        - test_size : 테스트 데이터의 비율 (0부터 1 사이의 값)\n",
    "        - random_state : 임의의 번호를 지정, 같은 숫자를 이용한다면 같은 값들을 출력 \n",
    "        - shuffle : True로 지정하면 추출하기 전에 데이터를 섞는다. \n",
    "        - stratify : None이 아닌 경우에는 지정된 변수를 기준으로 계층화를 하여 해당 변수의 비율이 유지 되도록 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 독립 변수들의 데이터 (2차원 배열)\n",
    "load_iris()['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 독립 변수들의 컬럼의 이름 (1차원 배열)\n",
    "load_iris()['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 종속 변수의 데이터 (1차원 배열)\n",
    "load_iris()['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.DataFrame(\n",
    "    load_iris()['data'], \n",
    "    columns = load_iris()['feature_names']\n",
    ")\n",
    "iris['class'] = load_iris()['target']\n",
    "iris['class'] = iris['class'].map(\n",
    "    {\n",
    "        0 : 'setosa', \n",
    "        1 : 'versicolour', \n",
    "        2 : 'virginaca'\n",
    "    }\n",
    ")\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할 \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    iris.drop('class', axis=1), \n",
    "    iris['class'], \n",
    "    test_size = 0.3, \n",
    "    random_state = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test의 개수를 확인 \n",
    "print('x_train 개수 : ', X_train.shape, 'x_test 개수 :', X_test.shape)\n",
    "print('y_train 개수 :', Y_train.shape, 'y_test 개수 :', Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 종속 변수의 데이터 분할이 랜덤하게 이루져있다. \n",
    "Y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris 데이터프레임에서 class 값을 기준으로 데이터를 7:3으로 나눠준다.\n",
    "X_train2, X_test2, Y_train2, Y_test2 = train_test_split(\n",
    "    iris.drop('class', axis=1), \n",
    "    iris['class'], \n",
    "    test_size = 0.3, \n",
    "    random_state = 100, \n",
    "    stratify = iris['class']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 스케일링 \n",
    "- 알고리즘 분석의 대부분은 컬럼 간의 데이터의 범위가 크게 차이가 나면 성능이 떨어지는 부분이 존재 \n",
    "- 값의 범위가 작은 컬럼에 비해서 값의 범위가 큰 컬럼이 타켓 변수를 예측하는데 큰 영향 \n",
    "- 스케일링은 모든 컬럼의 값의 범위를 같게 만들어주는 작업 \n",
    "- 스케일링 순서\n",
    "    - 주의할 점 : 데이터 스케일링은 train데이터와 test 데이터를 같은 scaler를 사용\n",
    "    1. Scaler 선택 및 로드 \n",
    "    2. Scaler 객체를 생성(Class 생성)\n",
    "    3. train 데이터의 분포를 저장 \n",
    "    4. teain 데이터를 스케일링\n",
    "    5. test 데이터를 스케일링\n",
    "    6. 원래의 스케일로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### standard Scaler\n",
    "- 표준화 방식으로 기본 스케일링 방식\n",
    "- 컬럼들의 데이터의 평균을 0, 분산이 1인 정규 분포로 스케일링\n",
    "- 회귀분석보다는 분류분석에서 유용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaler 로드 \n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaler 객체 생성 (Class 생성)\n",
    "StdScaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터의 분포 저장 \n",
    "StdScaler.fit(X_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train 데이터를 스케일링\n",
    "X_train_sc = StdScaler.transform(X_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 데이터 스케일링\n",
    "X_test_sc = StdScaler.transform(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(X_train_sc.min(), 2))\n",
    "print(round(X_train_sc.max(), 2))\n",
    "print(round(X_train_sc.mean(), 2))\n",
    "print(round(X_train_sc.std(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(X_test_sc.mean(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-Max Scaler\n",
    "- 정규화 방식으로 컬럼들을 0과 1사이의 값으로 스케일링 하는 방식\n",
    "- 최소값 : 0, 최대값 : 1\n",
    "- 이상치에 굉장히 민감함으로 이상치를 미리 확인하고 제거 \n",
    "- 분류 분석보다는 회귀 분석에서 주로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sclaer 로드 \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Scaler 객체 생성\n",
    "MmScaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data의 범위를 저장 \n",
    "MmScaler.fit(X_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data를 스케일링\n",
    "X_train_sc2 = MmScaler.transform(X_train2)\n",
    "# test data를 스케일링\n",
    "X_test_sc2 = MmScaler.transform(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(X_train_sc2.min(), 2))\n",
    "print(round(X_train_sc2.max(), 2))\n",
    "print(round(X_train_sc2.mean(), 2))\n",
    "print(round(X_train_sc2.std(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Abs Scaler\n",
    "- 최대절대값과 0을 기준으로 1과 0이 되도록 모든 값이 -1에서 1까지로 표현\n",
    "- 스케일링 데이터가 모두 양수인 경우에는 MinMaxScaler와 동일 \n",
    "- 이상치에 대해 굉장히 민감함으로 이상치를 제거후 스케일링\n",
    "- 분류 분석보다는 회귀 분석에서 유용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "MaScaler = MaxAbsScaler()\n",
    "MaScaler.fit(X_train2)\n",
    "X_train_sc3 = MaScaler.transform(X_train2)\n",
    "X_test_sc3 = MaScaler.transform(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(X_train_sc3.min(), 2))\n",
    "print(round(X_train_sc3.max(), 2))\n",
    "print(round(X_train_sc3.mean(), 2))\n",
    "print(round(X_train_sc3.std(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust Scaler \n",
    "- 중앙값과 사분위 값을 활용하여 스케일링 방식\n",
    "- 중앙값을 0으로 설정하고 IQR을 사용하여 이상치의 영향을 최소화하는 스케일링 방식\n",
    "- quantile_range 매개변수(기본값이 [0.25, 0.75])를 조정하여 더 넓거나 좁은 범위로 이상치를 설정하여 정제가 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "RuScaler = RobustScaler()\n",
    "\n",
    "RuScaler.fit(X_train2)\n",
    "\n",
    "X_train_sc4 = RuScaler.transform(X_train2)\n",
    "X_test_sc4 = RuScaler.transform(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중앙값을 확인 \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(X_train_sc4, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원본 데이터로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일링 데이터\n",
    "pd.DataFrame(X_train_sc4).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 데이터로 변환\n",
    "X_origin = RuScaler.inverse_transform(X_train_sc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_origin).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터의 불균형 문제 처리 \n",
    "- 정상을 정확하게 분류하는 것과 이상을 겅확하게 분류하는 것 중 일반적으로 이상을 정확하게 분류하는것이 중요\n",
    "- 일반적으로 이상 데이터가 target 값이 되는 경우가 많다. \n",
    "- 데이터가 불균형할 때는 분류의 성능과 target 데이터를 정확히 분류해내는 목표가 일치하지 않게 되는 현상이 발생 (성능이 떨어진다.)\n",
    "- 분석 결과 , 머신러닝 등 모델링에서 문제가 발생 \n",
    "- 소수의 데이터인 target의 중요도를 낮게 판단함으로 궁극적으로 분석 가능한 모델이 생성되지 않는다. \n",
    "- 소수의 비정상 데이터를 늘리는 오버샘플링, 상대적으로 많은 데이터에서 일부만 사용하는 언더샘플링이 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 언더샘플링 \n",
    "- 다수의 라벨을 가진 데이터를 샘플링하여 소수의 데이터셋이 가진 라벨의 수 수준으로 감소시키는 방법 \n",
    "- 데이터의 불균형 문제를 해결은 가능하지만, 데이터의 개수가 줄어듬으로 학습에 대한 성능이 떨어질수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 분포가 불균형한 데이터를 생성 ( 95: 1 )\n",
    "\n",
    "x, y = make_classification(n_samples=5000, n_features=5, \n",
    "                           weights=[0.95])\n",
    "print(Counter(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 랜덤언더샘플링은 다수를 차지하는 라벨에서 무작위로 데이터를 제거하는 방법\n",
    "- sampling_stratery 매개변수에 값을 majority로 지정하면 다수의 라벨의 데이터를 샘플링해서 소수의 라벨 데이터의 수와 같게 만든다. \n",
    "- 0과 1 사이의 값으로 지정하면 소수 라벨의 데이터 수와 다수 라벨의 데이터 수가 해당하는 비율에 맞게 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "x_under, y_under = undersample.fit_resample(x, y)\n",
    "print(Counter(y_under))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersample2 = RandomUnderSampler(sampling_strategy=0.5)\n",
    "x_under2, y_under2 = undersample2.fit_resample(x, y)\n",
    "print(Counter(y_under2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 오버샘플링 \n",
    "- 소수의 라벨을 지닌 데이터셋을 다수 라벨을 지닌 데이터셋의 수만큼 증식시켜서 학습에 사용하기 위한 충분한 양과 데이터를 확보하는 방법\n",
    "- 데이터의 손실이 없어서 일반적으로 언더샘플링보다는 성능이 유리하여 주로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 랜덤오버샘플링 \n",
    "- 소수의 라벨을 지닌 데이터셋을 단순 복제하여 다수의 라벨과 비율을 맞추는 방법\n",
    "- 데이터를 단순하게 복제하기 때문에 분포는 변하지 않지만 그 수가 증가함으로 같은 비율로 가중치를 받을수 있다. \n",
    "- 오버피팅의 위험성이 다분하지만 불균형 문제를 처리하지 않는것보다는 유효"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "oversample = RandomOverSampler(sampling_strategy=0.5)\n",
    "\n",
    "x_over, y_over = oversample.fit_resample(x, y)\n",
    "\n",
    "print(Counter(y_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample2 = RandomOverSampler(sampling_strategy='minority')\n",
    "x_over2, y_over2 = oversample2.fit_resample(x, y)\n",
    "\n",
    "print(Counter(y_over2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE\n",
    "- 소수 라벨을 지닌 데이터셋의 관측 값에 대한 K개의 최근접 이웃을 찾고 관측 값과 이웃으로 선택된 값 사이에 임의의 데이터를 생성하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote_sample = SMOTE(sampling_strategy='minority')\n",
    "x_sm , y_sm = smote_sample.fit_resample(x, y)\n",
    "print(Counter(y_sm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 2, ncols=2, figsize = (20, 20))\n",
    "\n",
    "sns.scatterplot(x = x[:, 1], y = x[:, 2], hue=y, ax=axes[0][0], alpha=0.5)\n",
    "sns.scatterplot(x = x_under[:, 1], y = x_under[:, 2], hue=y_under, ax=axes[0][1], alpha=0.5)\n",
    "sns.scatterplot(x = x_over2[:, 1], y = x_over2[:, 2], hue=y_over2, ax=axes[1][0], alpha=0.5)\n",
    "sns.scatterplot(x = x_sm[:, 1], y = x_sm[:, 2], hue=y_sm, ax=axes[1][1], alpha=0.5)\n",
    "\n",
    "axes[0][0].set_title('Original Data')\n",
    "axes[0][1].set_title('Random Under Sample')\n",
    "axes[1][0].set_title('Random Over Sample')\n",
    "axes[1][1].set_title('SMOTE')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
